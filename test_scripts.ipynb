{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'{' was never closed (evectools.py, line 232)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\Mateus\\Documents\\IC\\envIC\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3508\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 2\u001b[1;36m\n\u001b[1;33m    from evec_scan.tools import evectools as evt\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\Mateus\\Documents\\IC\\evec_scan\\tools\\evectools.py:232\u001b[1;36m\u001b[0m\n\u001b[1;33m    return {\"embedding_vector\":image_embedding\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m '{' was never closed\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from evec_scan.tools import evectools as evt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from keras import applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "instance_dir = 'evec_scan/instances/test1'\n",
    "vid_file = 'vid3.mp4'\n",
    "instance_id = 'test1'\n",
    "group_size = 2\n",
    "vidpath = 'evec_scan/instances/test1/vid3.mp4'\n",
    "description_models = ['moondream'] #['moondream', 'llava-llama3']\n",
    "description_embedding_models = ['mxbai-embed-large'] #['mxbai-embed-large','nomic-embed-text', 'all-minilm']\n",
    "#######################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files were succesfully saved\n"
     ]
    }
   ],
   "source": [
    "video_instance = evt.FrameExtractor(vidpath=vidpath)\n",
    "frame_group = video_instance.get_random_frame_group(size=group_size)\n",
    "frame_group_instance = video_instance.FrameGroup(frame_extractor=video_instance, group_set=frame_group)\n",
    "frames_output_path = os.path.join(instance_dir, 'frames')\n",
    "frame_path_list = frame_group_instance.save_group(output_dir=frames_output_path, group_name=instance_id)\n",
    "df['frame_path'] = frame_path_list\n",
    "df.index = list(range(len(frame_path_list)))\n",
    "########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for desc_model in description_models: \n",
    "    descriptions_per_model = []\n",
    "    description_columns = []\n",
    "    for frame_path in df['frame_path']:\n",
    "        descriptor = evt.ImgDescriptor(model=desc_model,frame_path=frame_path)\n",
    "        description = descriptor.get_description()\n",
    "        descriptions_per_model.append(description)\n",
    "    df[f'{desc_model}_desc'] = descriptions_per_model\n",
    "    description_columns.append(f'{desc_model}_desc')\n",
    "#######################################################################################################3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_path</th>\n",
       "      <th>moondream_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evec_scan/instances/test1\\frames\\test1_6522.jpg</td>\n",
       "      <td>\\nThe image depicts a city street at night, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evec_scan/instances/test1\\frames\\test1_31816.jpg</td>\n",
       "      <td>\\nThe image depicts a city street at night, wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         frame_path  \\\n",
       "0   evec_scan/instances/test1\\frames\\test1_6522.jpg   \n",
       "1  evec_scan/instances/test1\\frames\\test1_31816.jpg   \n",
       "\n",
       "                                      moondream_desc  \n",
       "0  \\nThe image depicts a city street at night, wi...  \n",
       "1  \\nThe image depicts a city street at night, wi...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in description_columns:\n",
    "    description_evec_output_dir = os.path.join(instance_dir, 'desc_evecs', dir)\n",
    "    os.makedirs(description_evec_output_dir)\n",
    "    for text_evec_model in description_embedding_models:\n",
    "        description_evec_arr_per_model_per_desc_model = np.empty(group_size, dtype=object)\n",
    "        i = 0\n",
    "        for description in df[dir]:\n",
    "            text_evec_scanner = evt.TextEvecScanner(model=text_evec_model, text=description)\n",
    "            text_evec = text_evec_scanner.get_evec()\n",
    "            description_evec_arr_per_model_per_desc_model[i]=text_evec\n",
    "            i += 1\n",
    "        output_path = os.path.join(description_evec_output_dir,f'{text_evec_model}.npy')\n",
    "        np.save(file=output_path, arr=description_evec_arr_per_model_per_desc_model)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the new models list is [<Functional name=vgg16, built=True>]\n",
      "models that will be used have indexes: [0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step\n",
      "the new models list is [<Functional name=densenet121, built=True>]\n",
      "models that will be used have indexes: [0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n"
     ]
    }
   ],
   "source": [
    "image_embedding_models = [applications.VGG16, applications.DenseNet121]\n",
    "for image_evec_models in image_embedding_models:\n",
    "    image_evec_arr_per_model_per_img_model = np.empty(group_size, dtype=object)\n",
    "    img_evec_scanner = evt.ImgEVecScanner()\n",
    "    img_evec_scanner.add_model(Keras_applications_model=image_evec_models)\n",
    "    for frame_path in df['frame_path']:\n",
    "        i = 0\n",
    "        res_dict = img_evec_scanner.get_models_evecs(frame_path=frame_path, lin_method='GAP')\n",
    "        for name, evec in zip(res_dict['model_name'],res_dict['embedding_vector']):\n",
    "            output_path_img_evec = os.path.join(instance_dir,f'{name}.npy')\n",
    "            image_evec_arr_per_model_per_img_model[i] = evec[0]\n",
    "            i += 1\n",
    "    np.save(file=output_path_img_evec, arr=image_evec_arr_per_model_per_img_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output_path = os.path.join(instance_dir, 'instance_data.csv')\n",
    "df.to_csv(df_output_path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('um', 'um2')\n",
      "('dois', 'dois2')\n"
     ]
    }
   ],
   "source": [
    "d = {\"a\":[\"um\", \"dois\"], \"b\":[\"um2\", \"dois2\"]}\n",
    "for x, y in zip(d[\"a\"],d[\"b\"]):\n",
    "    print((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evec_scan.tools import evectools as evt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ImgEVecScanner.get_clip_evec() missing 1 required positional argument: 'frame_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m m \u001b[38;5;241m=\u001b[39m evt\u001b[38;5;241m.\u001b[39mImgEVecScanner()\n\u001b[1;32m----> 2\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_clip_evec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevec_scan/instances/test1/frames/test1_848.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "\u001b[1;31mTypeError\u001b[0m: ImgEVecScanner.get_clip_evec() missing 1 required positional argument: 'frame_path'"
     ]
    }
   ],
   "source": [
    "m = evt.ImgEVecScanner()\n",
    "res = m.get_clip_evec( model=\"ViT-B/32\",frame_path=\"evec_scan/instances/test1/frames/test1_848.jpg\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envIC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
