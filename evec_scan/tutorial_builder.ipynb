{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passo1) aqui são importadas todas as bibliotecas e definidos todos os modelos que serão utilizados, além disso, manualmente aqui são indicados os paths do video que deverá passar pelo processo definido no script. \n",
    "- instance_dir = diretório da instância... usado como espaço de armazenamento para os folders, arrays e csv's resultantes do processo\n",
    "- instance_id = identificador unico da instância (aqui pode ser definido manualmente, mas é prudente que seja o mesmo nome da instância e que nomes de instancia não se repitam)\n",
    "- group_size = indica qual o numero de frames a serem extraídas do video (aleatoriamente uma vez que esta sendo usado o comando \"get_rando_frame_group\")\n",
    "- vidpath = path do video de onde os frames devem ser extraídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras import applications\n",
    "from tools import evectools as evt\n",
    "import pandas as pd\n",
    "from keras import applications\n",
    "\n",
    "\n",
    "############################This section contains all information necessary previous to the execution of the script#################################################################\n",
    "df = pd.DataFrame()\n",
    "instance_dir = 'C:/Documents/Projects_IC/creativision_nyu/UrbanCreativision/evec_scan/instances/test1'\n",
    "vid_file = 'video2.mp4'\n",
    "instance_id = 'test2'\n",
    "group_size = 6\n",
    "vidpath = 'C:/Documents/Projects_IC/creativision_nyu/UrbanCreativision/evec_scan/instances/test2/video2.mp4' \n",
    "description_models = ['moondream', 'llava-llama3'] #'moondream', 'llava-llama3'\n",
    "description_embedding_models = {\"model\":['mxbai-embed-large', \"ViT-B/32\"], \"model_origin\":['ollama', \"clip\"]} #'mxbai-embed-large','nomic-embed-text', 'all-minilm' from ollama\n",
    "image_embedding_models = {'model':[applications.VGG16, \"ViT-B/32\"], 'model_origin':['keras', 'clip']} #applications.VGG16, applications.DenseNet121\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passo2) esta próxima seção utiliza ferramentas definidas em evectools.py para extrair o grupo de frames e gerar uma coluna numa dataframe com todas as frames extraídas. No processo, usa \"save_group\" para gerar um folder onde cada frame é salva com seu respectivo nome e numero de identificação. Os paths dessas frames são so valores salvos no dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exists:  True\n",
      "{'streams': [{'index': 0, 'codec_name': 'h264', 'codec_long_name': 'H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10', 'profile': 'High', 'codec_type': 'video', 'codec_tag_string': 'avc1', 'codec_tag': '0x31637661', 'width': 1920, 'height': 1080, 'coded_width': 1920, 'coded_height': 1080, 'closed_captions': 0, 'film_grain': 0, 'has_b_frames': 0, 'pix_fmt': 'yuvj420p', 'level': 40, 'color_range': 'pc', 'color_space': 'bt470bg', 'color_transfer': 'smpte170m', 'color_primaries': 'bt470bg', 'chroma_location': 'left', 'field_order': 'progressive', 'refs': 1, 'is_avc': 'true', 'nal_length_size': '4', 'id': '0x1', 'r_frame_rate': '30/1', 'avg_frame_rate': '1113840000/37118533', 'time_base': '1/90000', 'start_pts': 0, 'start_time': '0.000000', 'duration_ts': 74237066, 'duration': '824.856289', 'bit_rate': '20006092', 'bits_per_raw_sample': '8', 'nb_frames': '24752', 'extradata_size': 35, 'disposition': {'default': 1, 'dub': 0, 'original': 0, 'comment': 0, 'lyrics': 0, 'karaoke': 0, 'forced': 0, 'hearing_impaired': 0, 'visual_impaired': 0, 'clean_effects': 0, 'attached_pic': 0, 'timed_thumbnails': 0, 'non_diegetic': 0, 'captions': 0, 'descriptions': 0, 'metadata': 0, 'dependent': 0, 'still_image': 0, 'multilayer': 0}, 'tags': {'creation_time': '2024-07-11T17:17:52.000000Z', 'language': 'eng', 'handler_name': 'VideoHandle', 'vendor_id': '[0][0][0][0]'}}, {'index': 1, 'codec_name': 'aac', 'codec_long_name': 'AAC (Advanced Audio Coding)', 'profile': 'LC', 'codec_type': 'audio', 'codec_tag_string': 'mp4a', 'codec_tag': '0x6134706d', 'sample_fmt': 'fltp', 'sample_rate': '48000', 'channels': 2, 'channel_layout': 'stereo', 'bits_per_sample': 0, 'initial_padding': 0, 'id': '0x2', 'r_frame_rate': '0/0', 'avg_frame_rate': '0/0', 'time_base': '1/48000', 'start_pts': 658, 'start_time': '0.013708', 'duration_ts': 39593251, 'duration': '824.859396', 'bit_rate': '155998', 'nb_frames': '38665', 'extradata_size': 2, 'disposition': {'default': 1, 'dub': 0, 'original': 0, 'comment': 0, 'lyrics': 0, 'karaoke': 0, 'forced': 0, 'hearing_impaired': 0, 'visual_impaired': 0, 'clean_effects': 0, 'attached_pic': 0, 'timed_thumbnails': 0, 'non_diegetic': 0, 'captions': 0, 'descriptions': 0, 'metadata': 0, 'dependent': 0, 'still_image': 0, 'multilayer': 0}, 'tags': {'creation_time': '2024-07-11T17:17:52.000000Z', 'language': 'eng', 'handler_name': 'SoundHandle', 'vendor_id': '[0][0][0][0]'}}], 'format': {'filename': 'C:/Documents/Projects_IC/creativision_nyu/UrbanCreativision/evec_scan/instances/test2/video2.mp4', 'nb_streams': 2, 'nb_programs': 0, 'nb_stream_groups': 0, 'format_name': 'mov,mp4,m4a,3gp,3g2,mj2', 'format_long_name': 'QuickTime / MOV', 'start_time': '0.000000', 'duration': '824.873104', 'size': '2079153285', 'bit_rate': '20164587', 'probe_score': 100, 'tags': {'major_brand': 'mp42', 'minor_version': '0', 'compatible_brands': 'isommp42', 'creation_time': '2024-07-11T17:17:52.000000Z', 'com.android.version': '14', 'com.android.manufacturer': 'motorola', 'com.android.model': 'motorola edge 30 neo'}}}\n",
      "files were succesfully saved\n"
     ]
    }
   ],
   "source": [
    "############################This section organizes a small group of frames from the video being analyzed###########################################################################\n",
    "video_instance = evt.FrameExtractor(vidpath=vidpath)\n",
    "frame_group = video_instance.get_random_frame_group(size=group_size)\n",
    "frame_group_instance = video_instance.FrameGroup(frame_extractor=video_instance, group_set=frame_group)\n",
    "frames_output_path = os.path.join(instance_dir, 'frames')\n",
    "frame_path_list = frame_group_instance.save_group(output_dir=frames_output_path, group_name=instance_id)\n",
    "df['frame_path'] = frame_path_list\n",
    "df.index = list(range(len(frame_path_list)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passo3) aqui são geradas as descrições de cada frame. A lógica utilizada é: Em ordem, para cada frame no dataframe, um primeiro modelo irá gerar a descrição. Na sequência, ele irá salvar essa descrição em uma nova coluna no dataframe, tal que, devido a ordem com que as descrições são geradas, alinhará cada frame com sua respectiva descrição feita por esse modelo. Na sequência um próximo modelo irá gerar as descrições de cada frame e gerar uma nova coluna. O nome da coluna é o nome do modelo usado. No final o dataframe resultante será salvo em um csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################This section gets the descriptions for every frame in the previous group model by model###########################################################################\n",
    "for desc_model in description_models: \n",
    "    descriptions_per_model = []\n",
    "    description_columns = []\n",
    "    for frame_path in df['frame_path']:\n",
    "        descriptor = evt.ImgDescriptor(model=desc_model,frame_path=frame_path)\n",
    "        description = descriptor.get_description()\n",
    "        descriptions_per_model.append(description)\n",
    "    df[f'{desc_model}_desc'] = descriptions_per_model\n",
    "    description_columns.append(f'{desc_model}_desc')\n",
    "df_output_path = os.path.join(instance_dir, 'instance_data.csv')\n",
    "df.to_csv(df_output_path, index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "segue o dataframe resultante do passo anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_path</th>\n",
       "      <th>moondream_desc</th>\n",
       "      <th>llava-llama3_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Documents/Projects_IC/creativision_nyu/Urba...</td>\n",
       "      <td>\\nThe image shows a gray car parked on the sid...</td>\n",
       "      <td>In the image, a silver hatchback car is captur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Documents/Projects_IC/creativision_nyu/Urba...</td>\n",
       "      <td>\\nThe image depicts a street scene with a blue...</td>\n",
       "      <td>In the image, there's a lively scene unfolding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Documents/Projects_IC/creativision_nyu/Urba...</td>\n",
       "      <td>\\nThe image shows a white car parked on the si...</td>\n",
       "      <td>The image captures a serene moment on a street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Documents/Projects_IC/creativision_nyu/Urba...</td>\n",
       "      <td>\\nThe image shows a bus stop on a street with ...</td>\n",
       "      <td>The image captures a serene scene in an urban ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Documents/Projects_IC/creativision_nyu/Urba...</td>\n",
       "      <td>\\nThe image shows a bus stop on the side of a ...</td>\n",
       "      <td>The image captures a scene from a street in So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C:/Documents/Projects_IC/creativision_nyu/Urba...</td>\n",
       "      <td>\\nThe image shows a bus stop on the side of a ...</td>\n",
       "      <td>The image captures a scene of urban tranquilit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          frame_path  \\\n",
       "0  C:/Documents/Projects_IC/creativision_nyu/Urba...   \n",
       "1  C:/Documents/Projects_IC/creativision_nyu/Urba...   \n",
       "2  C:/Documents/Projects_IC/creativision_nyu/Urba...   \n",
       "3  C:/Documents/Projects_IC/creativision_nyu/Urba...   \n",
       "4  C:/Documents/Projects_IC/creativision_nyu/Urba...   \n",
       "5  C:/Documents/Projects_IC/creativision_nyu/Urba...   \n",
       "\n",
       "                                      moondream_desc  \\\n",
       "0  \\nThe image shows a gray car parked on the sid...   \n",
       "1  \\nThe image depicts a street scene with a blue...   \n",
       "2  \\nThe image shows a white car parked on the si...   \n",
       "3  \\nThe image shows a bus stop on a street with ...   \n",
       "4  \\nThe image shows a bus stop on the side of a ...   \n",
       "5  \\nThe image shows a bus stop on the side of a ...   \n",
       "\n",
       "                                   llava-llama3_desc  \n",
       "0  In the image, a silver hatchback car is captur...  \n",
       "1  In the image, there's a lively scene unfolding...  \n",
       "2  The image captures a serene moment on a street...  \n",
       "3  The image captures a serene scene in an urban ...  \n",
       "4  The image captures a scene from a street in So...  \n",
       "5  The image captures a scene of urban tranquilit...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_data = pd.read_csv('C:/Documents/Projects_IC/creativision_nyu/UrbanCreativision/evec_scan/instances/test1/instance_data.csv')\n",
    "instance_data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passo4) usando as colunas de descrição do dataframe a próxima etapa consiste em, para cada uma dessas colunas, gerar um folder de ouput dentro de um outro folder chamado desc_evecs (de description embedding vectors). Esse folder terá o nome do modelo de descrição a partir do qual a descrição sob analise foi gerada. Dentro desse folder, será criado um numpy array .npy com o nome do modelo que gerou os embeddings. Nesse .npy estrão em ordem os vetores de embedding de cada frame do dataframe gerado anteriormente. Isso é feito para cada coluna de descrição e por cada modelo de embedding, tal que um folder desc_evecs/moondream_desc pode possuir multiplos .npy, como msbai-embed-large.npy entre outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################This section gets the embedding vector of each description model by model#########################################################################\n",
    "for dir in description_columns:\n",
    "    description_evec_output_dir = os.path.join(instance_dir, 'desc_evecs', dir)\n",
    "    os.makedirs(description_evec_output_dir)\n",
    "    for text_evec_model, model_origin in zip(description_embedding_models['model'],description_embedding_models['model_origin']):\n",
    "        description_evec_arr_per_model_per_desc_model = np.empty(group_size, dtype=object)\n",
    "        i = 0\n",
    "        for description in df[dir]:\n",
    "            text_evec_scanner = evt.TextEvecScanner(model=text_evec_model, text=description, model_origin=model_origin)\n",
    "            text_evec = text_evec_scanner.get_evec()\n",
    "            description_evec_arr_per_model_per_desc_model[i]=text_evec\n",
    "            i += 1\n",
    "        output_path_desc_evec = os.path.join(description_evec_output_dir,f'{str(text_evec_model).replace('/','')}.npy')\n",
    "        np.save(file=output_path_desc_evec, arr=description_evec_arr_per_model_per_desc_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passo5) para cada modelo de embedding de imagem, são gerados os vetores de embedding de cada frame e colocados dentro do .npy em ordem como no dataframe. Cada .npy tem o nome do modelo que o gerou e se encontra dentro do folder image_evecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################This section gets the embedding vector for each frame model by model##########################################################################\n",
    "img_evecs_output_dir = os.path.join(instance_dir, 'img_evecs')\n",
    "os.makedirs(img_evecs_output_dir)\n",
    "for image_evec_model, model_origin in zip(image_embedding_models['model'],image_embedding_models['model_origin']):\n",
    "    image_evec_arr_per_model_per_img_model = np.empty(group_size, dtype=object)\n",
    "    img_evec_scanner = evt.ImgEVecScanner()\n",
    "    if model_origin.lower() == \"keras\":\n",
    "        img_evec_scanner.add_model(Keras_applications_model=image_evec_model)\n",
    "    for frame_path in df['frame_path']:\n",
    "        i = 0\n",
    "        if model_origin.lower() == \"keras\":\n",
    "            res_dict = img_evec_scanner.get_models_evecs(frame_path=frame_path, lin_method='GAP')\n",
    "        elif model_origin.lower() == \"clip\":\n",
    "            res_dict = img_evec_scanner.get_clip_evec(model=image_evec_model, frame_path=frame_path)\n",
    "        for name, evec in zip(res_dict['model_name'],res_dict['embedding_vector']):\n",
    "            output_path_img_evec = os.path.join(img_evecs_output_dir,f'{str(name).replace('/','')}.npy')\n",
    "            image_evec_arr_per_model_per_img_model[i] = evec[0]\n",
    "            i += 1\n",
    "    np.save(file=output_path_img_evec, arr=image_evec_arr_per_model_per_img_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
